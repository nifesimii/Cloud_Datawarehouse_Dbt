# ğŸ¦ Banking Data Warehouse with Medallion Architecture

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![dbt](https://img.shields.io/badge/dbt-1.0+-orange.svg)](https://www.getdbt.com/)
[![AWS](https://img.shields.io/badge/AWS-Redshift-yellow.svg)](https://aws.amazon.com/redshift/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

> An end-to-end data warehouse implementation showcasing the Medallion Architecture (Bronze â†’ Silver â†’ Gold) for banking analytics using AWS Redshift, dbt, and Python.

## ğŸ“‹ Table of Contents
- [Project Overview](#-project-overview)
- [Architecture](#-architecture)
- [Tech Stack](#-tech-stack)
- [Data Model](#-data-model)
- [Project Structure](#-project-structure)
- [Setup & Installation](#-setup--installation)
- [Running the Pipeline](#-running-the-pipeline)
- [Sample Queries & Insights](#-sample-queries--insights)
- [Key Features](#-key-features)
- [Learning Outcomes](#-learning-outcomes)
- [Future Enhancements](#-future-enhancements)

---

## ğŸ¯ Project Overview

This project demonstrates a production-ready data warehouse solution for banking analytics, implementing the **Medallion Architecture** to transform raw transactional data into actionable business insights.

### Business Problem
Financial institutions need to analyze:
- Customer transaction patterns and behaviors
- Account balances and trends over time
- Investment portfolio performance
- Loan approval rates and risk assessment
- Channel effectiveness (Online, Mobile, ATM, In-Store)

### Solution
A scalable data warehouse that:
1. **Ingests** synthetic banking data (customers, transactions, investments, loans)
2. **Transforms** data through Bronze â†’ Silver â†’ Gold layers
3. **Enables** business intelligence and reporting through optimized dimensional models

---

## ğŸ—ï¸ Architecture

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Python Script  â”‚  Generate synthetic banking data
â”‚   (Faker lib)   â”‚  
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   AWS S3 Bucket â”‚  Raw CSV files storage
â”‚   (Bronze Layer)â”‚  
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AWS Glue       â”‚  Catalog & schema discovery
â”‚  Crawler        â”‚  
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AWS Redshift   â”‚  Data Warehouse
â”‚    Cluster      â”‚  
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   dbt Models    â”‚  Transform data (Silver â†’ Gold)
â”‚ Transformations â”‚  - Staging (Silver)
â”‚                 â”‚  - Marts (Gold)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    DBeaver      â”‚  Query & Visualization
â”‚  (SQL Client)   â”‚  Business Analytics
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Medallion Architecture Layers

| Layer | Schema | Purpose | Materialization |
|-------|--------|---------|-----------------|
| **ğŸ¥‰ Bronze** | `dev_bronze` | Raw data from S3 (Glue Catalog) | External Tables |
| **ğŸ¥ˆ Silver** | `dev_silver` | Cleaned, validated staging tables | Incremental Tables |
| **ğŸ¥‡ Gold** | `dev_gold` | Business-ready dimensional model | Tables (SCD Type 2) |

---

## ğŸ› ï¸ Tech Stack

### Data Generation
- **Python 3.8+** - Data generation scripts
- **Faker** - Synthetic data creation
- **CSV** - Initial data format

### Cloud Infrastructure
- **AWS S3** - Data lake storage
- **AWS Glue Crawler** - Metadata cataloging
- **AWS Redshift** - Columnar data warehouse

### Transformation & Modeling
- **dbt (Data Build Tool)** - SQL-based transformations
- **Jinja2** - Template logic in dbt models
- **SQL** - Core transformation language

### Analytics & Visualization
- **DBeaver** - SQL client for querying
- **SQL** - Ad-hoc analysis

---

## ğŸ“Š Data Model

### Dimensional Model (Star Schema)

#### **Dimension Tables**
```
ğŸ“ Dimensions (Silver & Gold Layers)
â”œâ”€â”€ dim_customer      - Customer profile information
â”œâ”€â”€ dim_date          - Date dimension (2020-2024)
â”œâ”€â”€ dim_channel       - Transaction channels (Online, Mobile, ATM, etc.)
â”œâ”€â”€ dim_account       - Account details and balances
â”œâ”€â”€ dim_location      - Geographic information
â”œâ”€â”€ dim_currency      - Currency codes (USD, EUR, JPY)
â”œâ”€â”€ dim_transaction_type - Transaction categories
â”œâ”€â”€ dim_investment_type  - Investment product types
â””â”€â”€ dim_loan          - Loan products and terms
```

#### **Fact Tables**
```
ğŸ“Š Facts (Silver & Gold Layers)
â”œâ”€â”€ fact_transaction           - Daily transaction activity
â”œâ”€â”€ fact_investment            - Investment performance tracking
â”œâ”€â”€ fact_loan                  - Loan applications and status
â”œâ”€â”€ fact_customer_interactions - Customer service interactions
â””â”€â”€ fact_daily_balances        - Account balance snapshots
```

### Entity Relationship Diagram

```
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ dim_customer â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   dim_date   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ dim_channel  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜         â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
            â”‚                â”‚                 â”‚
            â”‚                â”‚                 â”‚
            â”‚         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”          â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚    FACT     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            â”‚         â”‚ TRANSACTION â”‚          â”‚
            â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
            â”‚                â”‚                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”         â”‚         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚ dim_account  â”‚         â”‚         â”‚ dim_locationâ”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                     â”‚ dim_currency â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
dbt_redshift_dw/
â”œâ”€â”€ README.md                      # Project documentation
â”œâ”€â”€ main.py                        # Python data generation script
â”œâ”€â”€ dbt_project.yml                # dbt project configuration
â”œâ”€â”€ macros/
â”‚   â””â”€â”€ get_custom_schema.sql     # Custom schema naming macro
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ silver/                   # Staging layer
â”‚   â”‚   â”œâ”€â”€ dimensions/
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_dim_customer.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_dim_date.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_dim_account.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_dim_channel.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_dim_currency.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_dim_investment_type.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_dim_location.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_dim_loan.sql
â”‚   â”‚   â”‚   â””â”€â”€ stg_dim_transaction_type.sql
â”‚   â”‚   â””â”€â”€ facts/
â”‚   â”‚       â”œâ”€â”€ stg_fact_transaction.sql
â”‚   â”‚       â”œâ”€â”€ stg_fact_investment.sql
â”‚   â”‚       â”œâ”€â”€ stg_fact_loan.sql
â”‚   â”‚       â”œâ”€â”€ stg_fact_customer_interactions.sql
â”‚   â”‚       â””â”€â”€ stg_fact_daily_balances.sql
â”‚   â””â”€â”€ gold/                     # Business layer
â”‚       â”œâ”€â”€ dimensions/
â”‚       â”‚   â”œâ”€â”€ dim_customer.sql
â”‚       â”‚   â”œâ”€â”€ dim_date.sql
â”‚       â”‚   â””â”€â”€ ...
â”‚       â””â”€â”€ facts/
â”‚           â”œâ”€â”€ fact_transaction.sql
â”‚           â”œâ”€â”€ fact_investment.sql
â”‚           â””â”€â”€ ...
â”œâ”€â”€ tests/                        # dbt tests
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ silver/
â”‚   â””â”€â”€ gold/
â”œâ”€â”€ seeds/                        # Static reference data
â”œâ”€â”€ snapshots/                    # SCD Type 2 tracking
â””â”€â”€ analyses/                     # Ad-hoc analysis queries
```

---

## ğŸš€ Setup & Installation

### Prerequisites
- Python 3.8+
- AWS Account with Redshift cluster
- dbt installed (`pip install dbt-redshift`)
- AWS CLI configured
- DBeaver or any SQL client

### 1. Clone the Repository
```bash
git clone https://github.com/yourusername/dbt-redshift-banking-dw.git
cd dbt-redshift-banking-dw
```

### 2. Set Up Python Environment
```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

**requirements.txt:**
```
faker==20.0.0
boto3==1.28.0
python-dotenv==1.0.0
```

### 3. Generate Synthetic Data
```bash
python main.py
```

This generates CSV files:
- `dim_customers.csv` (100 records)
- `dim_dates.csv` (5 years: 2020-2024)
- `dim_channels.csv` (5 channels)
- `dim_transaction_types.csv` (4 types)
- `dim_locations.csv` (50 locations)
- `dim_currencies.csv` (3 currencies)
- `dim_accounts.csv` (100 accounts)
- `dim_investment_types.csv` (5 types)
- `dim_loans.csv` (50 loans)
- `fact_transactions.csv` (10,000 transactions)
- `fact_investments.csv` (10,000 investments)
- `fact_loans.csv` (10,000 loan applications)
- `fact_customer_interactions.csv` (10,000 interactions)
- `fact_daily_balances.csv` (10,000 balance snapshots)

### 4. Upload to S3
```bash
aws s3 sync . s3://your-bucket-name/banking-data/ --exclude "*" --include "*.csv"
```

### 5. Configure AWS Glue Crawler
1. Navigate to AWS Glue Console
2. Create a new Crawler pointing to your S3 bucket
3. Set target database: `dev_bronze`
4. Run the crawler to catalog all CSV files

### 6. Set Up Redshift
```sql
-- Create schemas
CREATE SCHEMA IF NOT EXISTS dev_bronze;
CREATE SCHEMA IF NOT EXISTS dev_silver;
CREATE SCHEMA IF NOT EXISTS dev_gold;

-- Create external schema for Glue Catalog
CREATE EXTERNAL SCHEMA bronze_external
FROM DATA CATALOG
DATABASE 'dev_bronze'
IAM_ROLE 'arn:aws:iam::YOUR_ACCOUNT_ID:role/RedshiftS3Role'
REGION 'us-east-1';
```

### 7. Configure dbt Profile
Edit `~/.dbt/profiles.yml`:
```yaml
dbt_redshift_dw:
  outputs:
    dev:
      type: redshift
      host: your-cluster.region.redshift.amazonaws.com
      port: 5439
      user: your_username
      password: your_password
      dbname: dev
      schema: dev_gold
      threads: 4
  target: dev
```

### 8. Install dbt Dependencies
```bash
dbt deps
```

---

## â–¶ï¸ Running the Pipeline

### Full Pipeline Run
```bash
# Run all models (Silver + Gold layers)
dbt run

# Run with full refresh (rebuild all tables)
dbt run --full-refresh

# Run specific layer
dbt run --select silver.*
dbt run --select gold.*

# Run specific model
dbt run --select stg_dim_customer
```

### Testing
```bash
# Run all tests
dbt test

# Test specific layer
dbt test --select silver.*

# Test specific model
dbt test --select dim_customer
```

### Documentation
```bash
# Generate documentation
dbt docs generate

# Serve documentation locally
dbt docs serve
```

### Typical Workflow
```bash
# 1. Generate fresh data
python main.py

# 2. Upload to S3
aws s3 sync . s3://your-bucket/banking-data/ --include "*.csv"

# 3. Run Glue Crawler
aws glue start-crawler --name banking-data-crawler

# 4. Run dbt transformations
dbt run

# 5. Test data quality
dbt test

# 6. Query in DBeaver
```

---

## ğŸ“ˆ Sample Queries & Insights

### 1. Top 10 Customers by Transaction Volume
```sql
SELECT 
    c.customer_id,
    c.first_name || ' ' || c.last_name AS customer_name,
    COUNT(ft.transaction_id) AS total_transactions,
    SUM(ft.transaction_amount) AS total_amount,
    AVG(ft.transaction_amount) AS avg_transaction
FROM dev_gold.dim_customer c
JOIN dev_gold.dim_account a ON c.customer_id = a.customer_id
JOIN dev_gold.fact_transaction ft ON a.account_id = ft.account_id
WHERE ft.transaction_status = 'Completed'
GROUP BY 1, 2
ORDER BY total_amount DESC
LIMIT 10;
```

### 2. Monthly Transaction Trends by Channel
```sql
SELECT 
    d.year,
    d.month,
    ch.channel_name,
    COUNT(ft.transaction_id) AS transaction_count,
    SUM(ft.transaction_amount) AS total_amount
FROM dev_gold.fact_transaction ft
JOIN dev_gold.dim_date d ON ft.date_id = d.date_id
JOIN dev_gold.dim_channel ch ON ft.channel_id = ch.channel_id
WHERE ft.transaction_status = 'Completed'
GROUP BY 1, 2, 3
ORDER BY 1, 2, 4 DESC;
```

### 3. Investment Performance by Type
```sql
SELECT 
    it.investment_type_name,
    COUNT(fi.investment_id) AS total_investments,
    SUM(fi.investment_amount) AS total_invested,
    SUM(fi.investment_return) AS total_return,
    ROUND(
        (SUM(fi.investment_return) / NULLIF(SUM(fi.investment_amount), 0)) * 100, 
        2
    ) AS roi_percentage
FROM dev_gold.fact_investment fi
JOIN dev_gold.dim_investment_type it ON fi.investment_type_id = it.investment_type_id
GROUP BY 1
ORDER BY roi_percentage DESC;
```

### 4. Loan Approval Rates by Type
```sql
SELECT 
    dl.loan_type,
    COUNT(fl.loan_fact_id) AS total_applications,
    SUM(CASE WHEN fl.loan_status = 'Approved' THEN 1 ELSE 0 END) AS approved,
    SUM(CASE WHEN fl.loan_status = 'Rejected' THEN 1 ELSE 0 END) AS rejected,
    ROUND(
        (SUM(CASE WHEN fl.loan_status = 'Approved' THEN 1 ELSE 0 END)::FLOAT / 
         COUNT(fl.loan_fact_id)) * 100, 
        2
    ) AS approval_rate
FROM dev_gold.fact_loan fl
JOIN dev_gold.dim_loan dl ON fl.loan_id = dl.loan_id
GROUP BY 1
ORDER BY approval_rate DESC;
```

### 5. Customer Balance Trends Over Time
```sql
WITH daily_totals AS (
    SELECT 
        d.date,
        SUM(fdb.closing_balance) AS total_balance,
        AVG(fdb.closing_balance) AS avg_customer_balance,
        COUNT(DISTINCT fdb.account_id) AS active_accounts
    FROM dev_gold.fact_daily_balances fdb
    JOIN dev_gold.dim_date d ON fdb.date_id = d.date_id
    GROUP BY 1
)
SELECT *
FROM daily_totals
ORDER BY date DESC
LIMIT 30;
```

### 6. Geographic Transaction Distribution
```sql
SELECT 
    l.country,
    l.state,
    l.city,
    COUNT(ft.transaction_id) AS transaction_count,
    SUM(ft.transaction_amount) AS total_amount,
    AVG(ft.transaction_amount) AS avg_amount
FROM dev_gold.fact_transaction ft
JOIN dev_gold.dim_location l ON ft.location_id = l.location_id
WHERE ft.transaction_status = 'Completed'
GROUP BY 1, 2, 3
ORDER BY total_amount DESC
LIMIT 20;
```

---

## âœ¨ Key Features

### 1. Medallion Architecture Implementation
- **Bronze Layer**: Raw data from S3 via Glue Catalog
- **Silver Layer**: Cleaned, validated, incremental staging tables
- **Gold Layer**: Business-ready dimensional models with SCD Type 2

### 2. Data Quality & Testing
- Primary key uniqueness tests
- Not-null constraints on critical fields
- Referential integrity checks
- Custom dbt tests for business logic

### 3. Incremental Loading
```sql
-- Example: Incremental transaction loading
{{ config(
    materialized='incremental',
    unique_key='transaction_id',
    on_schema_change='fail'
) }}

SELECT *
FROM {{ source('bronze', 'fact_transactions') }}
{% if is_incremental() %}
WHERE date_id > (SELECT MAX(date_id) FROM {{ this }})
{% endif %}
```

### 4. Custom Schema Management
```sql
-- macros/get_custom_schema.sql
{% macro generate_schema_name(custom_schema_name, node) -%}
    {%- set default_schema = target.schema -%}
    {%- if custom_schema_name is none -%}
        {{ default_schema }}
    {%- else -%}
        {{ custom_schema_name | trim }}
    {%- endif -%}
{%- endmacro %}
```

### 5. Performance Optimization
- Redshift distribution keys on high-cardinality columns
- Sort keys on date columns for time-series queries
- Compression encoding for columnar storage
- Incremental models to reduce processing time

---

## ğŸ“ Learning Outcomes

This project demonstrates proficiency in:

### Data Engineering
- âœ… End-to-end data pipeline development
- âœ… Medallion architecture implementation
- âœ… Cloud data warehouse design (AWS Redshift)
- âœ… ETL/ELT best practices

### dbt Mastery
- âœ… dbt models (staging, intermediate, marts)
- âœ… Incremental loading strategies
- âœ… Custom macros and Jinja templating
- âœ… Data testing and documentation

### AWS Ecosystem
- âœ… S3 for data lake storage
- âœ… Glue Crawler for metadata management
- âœ… Redshift cluster configuration
- âœ… IAM roles and permissions

### SQL & Analytics
- âœ… Complex SQL queries and window functions
- âœ… Star schema dimensional modeling
- âœ… Performance optimization techniques
- âœ… Business intelligence query patterns

### Software Engineering
- âœ… Version control with Git
- âœ… Python scripting for data generation
- âœ… Documentation and README best practices
- âœ… Code modularity and reusability

---

## ğŸ”® Future Enhancements

### Technical Improvements
- [ ] Implement CI/CD pipeline (GitHub Actions)
- [ ] Add Airflow for orchestration
- [ ] Implement data lineage tracking
- [ ] Add real-time streaming with Kafka/Kinesis
- [ ] Implement data quality monitoring (Great Expectations)
- [ ] Add dbt snapshot models for SCD Type 2

### Analytics Enhancements
- [ ] Build Tableau/Power BI dashboards
- [ ] Implement customer segmentation (RFM analysis)
- [ ] Add predictive models (loan default, churn)
- [ ] Create anomaly detection for fraud
- [ ] Build customer lifetime value models

### Infrastructure Upgrades
- [ ] Terraform/CloudFormation for IaC
- [ ] Implement data catalog (AWS DataZone/DataHub)
- [ ] Add data masking for PII compliance
- [ ] Implement disaster recovery strategy
- [ ] Add cost optimization (spot instances, reserved capacity)

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ¤ Contributing

Contributions, issues, and feature requests are welcome! Feel free to check the [issues page](https://github.com/yourusername/dbt-redshift-banking-dw/issues).

---

## ğŸ‘¤ Author

**Nifesimi Ademoye**

- LinkedIn: [Your LinkedIn](https://linkedin.com/in/yourprofile)
- GitHub: [@yourusername](https://github.com/yourusername)
- Portfolio: [yourportfolio.com](https://yourportfolio.com)

---

## ğŸ™ Acknowledgments

- AWS for cloud infrastructure
- dbt Labs for the transformation framework
- Faker library for synthetic data generation
- The data engineering community for best practices

---

## ğŸ“š References

- [dbt Documentation](https://docs.getdbt.com/)
- [AWS Redshift Best Practices](https://docs.aws.amazon.com/redshift/latest/dg/best-practices.html)
- [Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture)
- [Dimensional Modeling (Kimball)](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/)

---

â­ **If you found this project helpful, please consider giving it a star!** â­
